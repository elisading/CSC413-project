{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/gracewu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gracewu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(column):\n",
    "    \"\"\"Tokenizes a Pandas dataframe column and returns a list of tokens.\n",
    "\n",
    "    Args:\n",
    "        column: Pandas dataframe column (i.e. df['text']).\n",
    "\n",
    "    Returns:\n",
    "        tokens (list): Tokenized list, i.e. [Donald, Trump, tweets]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = nltk.word_tokenize(column)\n",
    "    return [w for w in tokens if w.isalpha()]  \n",
    "\n",
    "def remove_stopwords(tokenized_column):\n",
    "    \"\"\"Return a list of tokens with English stopwords removed. \n",
    "\n",
    "    Args:\n",
    "        column: Pandas dataframe column of tokenized data from tokenize()\n",
    "\n",
    "    Returns:\n",
    "        tokens (list): Tokenized list with stopwords removed.\n",
    "\n",
    "    \"\"\"\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    return [word for word in tokenized_column if not word in stops]\n",
    "\n",
    "def apply_stemming(tokenized_column):\n",
    "    \"\"\"Return a list of tokens with Porter stemming applied.\n",
    "\n",
    "    Args:\n",
    "        column: Pandas dataframe column of tokenized data with stopwords removed.\n",
    "\n",
    "    Returns:\n",
    "        tokens (list): Tokenized list with words Porter stemmed.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = PorterStemmer() \n",
    "    return [stemmer.stem(word) for word in tokenized_column]\n",
    "\n",
    "def rejoin_words(tokenized_column):\n",
    "    \"\"\"Rejoins a tokenized word list into a single string. \n",
    "    \n",
    "    Args:\n",
    "        tokenized_column (list): Tokenized column of words. \n",
    "        \n",
    "    Returns:\n",
    "        string: Single string of untokenized words. \n",
    "    \"\"\"\n",
    "    \n",
    "    return ( \" \".join(tokenized_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"./amazon1.csv\")\n",
    "cols1 = ['Uniq Id', 'Product Name', 'Category', 'About Product', 'Image']\n",
    "prod_df1 = df1[cols1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dropna()\n",
    "for j in range(1, len(cols1)-1):\n",
    "    df1['tokenized'] = df1.apply(lambda x: tokenize(str(x[cols1[j]])), axis=1)\n",
    "    df1['stopwords_removed'] = df1.apply(lambda x: remove_stopwords(x['tokenized']), axis=1)\n",
    "    df1['porter_stemmed'] = df1.apply(lambda x: apply_stemming(x['stopwords_removed']), axis=1)\n",
    "    df1['rejoined'] = df1.apply(lambda x: rejoin_words(x['porter_stemmed']), axis=1)\n",
    "    df1[cols1[j]] = df1['rejoined']\n",
    "    df1 = df1.drop(['tokenized','stopwords_removed', 'porter_stemmed', 'rejoined'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"About Product\"] = df1.apply(lambda x: rejoin_words(tokenize(str(x['About Product']))[6:]), axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting images for just the first dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Image'] = df1.apply(lambda x: str(x['Image']).split('|')[:-1], axis=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.explode(\"Image\", ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Image'] = df1['Image'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.index.name = 'ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "exception = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(df1)):\n",
    "    url = df1[\"Image\"][i]\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        img = img.resize((512,512))\n",
    "        img.save(\"./amazon_img/\" + str(i) + \".jpg\")\n",
    "    except:\n",
    "        exception += [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33993"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from pathlib import Path\n",
    "\n",
    "exception = []\n",
    "for i in range(0, len(prod_df[0])):\n",
    "    my_file = Path(\"./amazon_img/\" + str(i) + '.jpg')\n",
    "    my_file2 = Path(\"./toy_img/\" + str(i) + '.jpg')\n",
    "    if not my_file.is_file() and not my_file2.is_file():\n",
    "        exception += [i]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop(exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33744"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['text'] = df1[[\"Product Name\", \"Category\", \"About Product\" ]].apply(\" \".join, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate text column, make categories column a list so we can filter it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[cols1[2]] = df1.apply(lambda x: tokenize(str(x[cols1[2]])), axis=1)\n",
    "df1['temp'] = df1.apply(lambda x: rejoin_words(x[cols1[2]]), axis=1)\n",
    "df1['text'] = df1[1:4].apply(\" \".join, axis=1)\n",
    "df1 = df1.drop(['temp'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code for filtering dataset based on category\n",
    "filter_cat = \"toy\"\n",
    "mask = df1[\"Category\"].apply(lambda x: filter_cat in x)\n",
    "masked = df1[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23024"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(masked)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
